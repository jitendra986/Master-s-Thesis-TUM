{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(dset=1):\n",
    "\n",
    "    # Read data file\n",
    "\n",
    "    dtype='train'\n",
    "    file_name = '{}_FD00{}.txt'.format(dtype, str(dset))\n",
    "    columns = ['unit', 'cycle', 'os1', 'os2', 'os3'] + ['sm{}'.format(j+1) for j in range(23)]\n",
    "    \n",
    "    data = pd.read_csv('./CMAPSSData/{}'.format(file_name), delimiter=' ', names=columns)\n",
    "    \n",
    "        \n",
    "    # Remove the last two columns\n",
    "    \n",
    "    data.drop(data.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Calculate end-of-life cycles for each unit\n",
    "    \n",
    "    eol_cycles = pd.DataFrame(data.groupby('unit')['cycle'].max()).reset_index()\n",
    "    eol_cycles.columns = ['unit', 'eol_cycles']\n",
    "    \n",
    "    \n",
    "    # Calculate Remaining Useful Life (RUL) for each unit at each cycle\n",
    "\n",
    "    data = data.merge(eol_cycles, on=['unit'], how='left')\n",
    "    data['RUL'] = data['eol_cycles'] - data['cycle']\n",
    "    data.drop('eol_cycles', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Create label columns\n",
    "    w = 10\n",
    "    data['L1'] = np.where(data['RUL'] < w, 1, 0)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readDataset(dset=1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window = 5\n",
    "data_smoothed = data.copy()\n",
    "\n",
    "for unit in np.arange(1, 101, 1):\n",
    "    for channel in np.arange(1, 22, 1): #measurement_channels:\n",
    "        smoothed_data = data.loc[data['unit'] == unit, 'sm{}'.format(channel)].rolling(smoothing_window).sum()/smoothing_window\n",
    "        data_smoothed.loc[data_smoothed['unit'] == unit, 'sm{}'.format(channel)] = smoothed_data\n",
    "        \n",
    "data_smoothed = data_smoothed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the training set: 194/15818 (1.23%)\n",
      "\n",
      "Number of mislabeled points in the validation set: 86/4413 (1.95%)\n"
     ]
    }
   ],
   "source": [
    "measurement_channels = [7, 11, 14]\n",
    "\n",
    "training_units = np.arange(1, 81, 1)\n",
    "validation_units = np.array(list(set(np.arange(1, 101, 1)).difference(training_units)))\n",
    "\n",
    "data_shift = 1 # Change this for N-step ahead prediction\n",
    "\n",
    "\n",
    "def sliceData(data, channels, units):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get a slice of the data set corresponding to specific channels and units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "        The data to be sliced.\n",
    "    channels: list, tuple or ndarray\n",
    "        The list of channels to be sliced.\n",
    "    units: list, tuple of ndarray\n",
    "        The list of units to be sliced.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The sliced input and output data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    sliced_data = data.loc[data['unit'].isin(units)]\n",
    "    X = sliced_data[['sm{}'.format(j) for j in channels]]\n",
    "    Y = sliced_data['L1']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def shiftData(X, Y, shift):\n",
    "\n",
    "    \"\"\"\n",
    "    Shift input and output data for N-step ahead predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The input and output data sets.\n",
    "    shift: int\n",
    "        The number of shifts\n",
    "    \"\"\"\n",
    "\n",
    "    X = X[:-shift].to_numpy()\n",
    "    Y = Y[shift:].to_numpy()\n",
    "    \n",
    "    rows_to_delete = np.where(Y == 1)[0][:-1] + data_shift\n",
    "    X = np.delete(X, rows_to_delete, axis=0)\n",
    "    Y = np.delete(Y, rows_to_delete)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "training_data = data_smoothed.loc[data_smoothed['unit'].isin(training_units)]\n",
    "validation_data = data_smoothed.loc[~data_smoothed['unit'].isin(training_units)]\n",
    "\n",
    "X_training, Y_training = sliceData(data_smoothed, measurement_channels, training_units)\n",
    "X_validation, Y_validation = sliceData(data_smoothed, measurement_channels, validation_units)\n",
    "\n",
    "transformer = MinMaxScaler().fit(X_training)\n",
    "# transformer = MaxAbsScaler().fit(X_training)\n",
    "X_training = transformer.transform(X_training)\n",
    "X_validation = transformer.transform(X_validation)\n",
    "\n",
    "\n",
    "# Train and evaluate the model using the training set\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion='entropy',max_depth=7)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# dtc = tree.DecisionTreeClassifier(max_depth=1)\n",
    "Y_predicted = dtc.fit(X_training, Y_training).predict(X_training)\n",
    "\n",
    "total = X_training.shape[0]\n",
    "mislabeled = (Y_training != Y_predicted).sum()\n",
    "message = 'Number of mislabeled points in the training set: {}/{} ({:.2f}%)\\n'\n",
    "print(message.format(mislabeled, total, mislabeled/total*100))\n",
    "\n",
    "# Evaluate the model using the validation set\n",
    "\n",
    "Y_predicted = dtc.predict(X_validation)\n",
    "\n",
    "total = X_validation.shape[0]\n",
    "mislabeled = (Y_validation != Y_predicted).sum()\n",
    "message = 'Number of mislabeled points in the validation set: {}/{} ({:.2f}%)'\n",
    "print(message.format(mislabeled, total, mislabeled/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles\n",
    "C_p = 100\n",
    "DT  = 10  # Decisions can be taken every DT=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizer_training_set(C_c):\n",
    "    \n",
    "    print('C_c=', C_c)\n",
    "    costs_array = np.zeros(80)\n",
    "    t_LC_array  = np.zeros(80)\n",
    "\n",
    "    counter = 0\n",
    "    PR_thres = C_p/C_c\n",
    "\n",
    "    for unit in training_data['unit'].unique():\n",
    "\n",
    "        preventive_replacement = False\n",
    "\n",
    "        X = training_data[['sm{}'.format(j) for j in measurement_channels]].loc[training_data['unit'] == unit].to_numpy()\n",
    "        X = transformer.transform(X)\n",
    "        Y = training_data['L1'].loc[training_data['unit'] == unit].to_numpy()\n",
    "\n",
    "        for cycle in range(training_data[training_data['unit']==unit].shape[0]):\n",
    "\n",
    "            if smoothing_window + cycle in array_decisions:\n",
    "\n",
    "                prob_RUL_smaller_DT = dtc.predict_proba(X[cycle].reshape(1,3))[0,1]\n",
    "#                 print(prob_RUL_smaller_DT)\n",
    "\n",
    "    #             if prob_RUL_smaller_DT < 0.5:\n",
    "    #                 prob_RUL_smaller_DT = 0\n",
    "\n",
    "                # evaluate decision heuristics\n",
    "                if PR_thres <= prob_RUL_smaller_DT:\n",
    "\n",
    "                    t_LC_array[counter] = smoothing_window+cycle\n",
    "                    costs_array[counter] = C_p\n",
    "#                     print('Unit:', unit, ', preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "#                     print('component lifecycle:', t_LC_array[counter])\n",
    "                    preventive_replacement = True\n",
    "                    break\n",
    "\n",
    "        if preventive_replacement == False:\n",
    "\n",
    "            t_LC_array[counter] = training_data[training_data['unit']==unit]['cycle'].iloc[-1]\n",
    "            print('Unit:', unit, ', component failure at t:', t_LC_array[counter])\n",
    "            costs_array[counter] = C_c\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "    expected_cost = np.mean(costs_array) / np.mean(t_LC_array)   # this is the objective function\n",
    "    return expected_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_c = np.array([10000, 5000, 2000, 1000, 500, 300, 200, 165, 150, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_c= 10000\n",
      "C_c= 5000\n",
      "C_c= 2000\n",
      "C_c= 1000\n",
      "C_c= 500\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "C_c= 300\n",
      "Unit: 24 , component failure at t: 147.0\n",
      "Unit: 57 , component failure at t: 137.0\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "C_c= 200\n",
      "Unit: 24 , component failure at t: 147.0\n",
      "Unit: 27 , component failure at t: 156.0\n",
      "Unit: 45 , component failure at t: 158.0\n",
      "Unit: 57 , component failure at t: 137.0\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "C_c= 165\n",
      "Unit: 7 , component failure at t: 259.0\n",
      "Unit: 23 , component failure at t: 168.0\n",
      "Unit: 24 , component failure at t: 147.0\n",
      "Unit: 27 , component failure at t: 156.0\n",
      "Unit: 36 , component failure at t: 158.0\n",
      "Unit: 42 , component failure at t: 196.0\n",
      "Unit: 45 , component failure at t: 158.0\n",
      "Unit: 57 , component failure at t: 137.0\n",
      "Unit: 58 , component failure at t: 147.0\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "Unit: 74 , component failure at t: 166.0\n",
      "C_c= 150\n",
      "Unit: 7 , component failure at t: 259.0\n",
      "Unit: 10 , component failure at t: 222.0\n",
      "Unit: 15 , component failure at t: 207.0\n",
      "Unit: 16 , component failure at t: 209.0\n",
      "Unit: 23 , component failure at t: 168.0\n",
      "Unit: 24 , component failure at t: 147.0\n",
      "Unit: 27 , component failure at t: 156.0\n",
      "Unit: 30 , component failure at t: 194.0\n",
      "Unit: 36 , component failure at t: 158.0\n",
      "Unit: 42 , component failure at t: 196.0\n",
      "Unit: 43 , component failure at t: 207.0\n",
      "Unit: 45 , component failure at t: 158.0\n",
      "Unit: 50 , component failure at t: 198.0\n",
      "Unit: 57 , component failure at t: 137.0\n",
      "Unit: 58 , component failure at t: 147.0\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "Unit: 74 , component failure at t: 166.0\n",
      "Unit: 75 , component failure at t: 229.0\n",
      "Unit: 79 , component failure at t: 199.0\n",
      "C_c= 120\n",
      "Unit: 7 , component failure at t: 259.0\n",
      "Unit: 10 , component failure at t: 222.0\n",
      "Unit: 15 , component failure at t: 207.0\n",
      "Unit: 16 , component failure at t: 209.0\n",
      "Unit: 23 , component failure at t: 168.0\n",
      "Unit: 24 , component failure at t: 147.0\n",
      "Unit: 27 , component failure at t: 156.0\n",
      "Unit: 30 , component failure at t: 194.0\n",
      "Unit: 36 , component failure at t: 158.0\n",
      "Unit: 42 , component failure at t: 196.0\n",
      "Unit: 43 , component failure at t: 207.0\n",
      "Unit: 45 , component failure at t: 158.0\n",
      "Unit: 50 , component failure at t: 198.0\n",
      "Unit: 57 , component failure at t: 137.0\n",
      "Unit: 58 , component failure at t: 147.0\n",
      "Unit: 70 , component failure at t: 137.0\n",
      "Unit: 74 , component failure at t: 166.0\n",
      "Unit: 75 , component failure at t: 229.0\n",
      "Unit: 79 , component failure at t: 199.0\n"
     ]
    }
   ],
   "source": [
    "expected_cost_on_grid = np.zeros(np.size(C_c))\n",
    "\n",
    "for i in range(np.size(C_c)):\n",
    "    expected_cost_on_grid[i] = minimizer_training_set(C_c[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5242464 , 0.5242464 , 0.52321779, 0.51948052, 0.53994986,\n",
       "       0.55018873, 0.54226475, 0.55091978, 0.56310557, 0.52724298])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_cost_on_grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cost_perfect = 0.507292327203551 # for the 80 training data points, I computed in the LSTM notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.34207077,  3.34207077,  3.13930674,  2.4025974 ,  6.43761651,\n",
       "        8.45595291,  6.89393939,  8.60006163, 11.00218636,  3.93277337])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = (expected_cost_on_grid-expected_cost_perfect)/expected_cost_perfect *100 # optimal 0.9\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "name": "Predictive-Maintenance-using-LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
