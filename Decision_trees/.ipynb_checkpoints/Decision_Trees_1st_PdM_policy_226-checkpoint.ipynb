{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "from scipy.optimize import least_squares, curve_fit, leastsq\n",
    "import scipy.integrate as integrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset(dset=1):\n",
    "\n",
    "    # Read data file\n",
    "\n",
    "    dtype='train'\n",
    "    file_name = '{}_FD00{}.txt'.format(dtype, str(dset))\n",
    "    columns = ['unit', 'cycle', 'os1', 'os2', 'os3'] + ['sm{}'.format(j+1) for j in range(23)]\n",
    "    \n",
    "    data = pd.read_csv('./CMAPSSData/{}'.format(file_name), delimiter=' ', names=columns)\n",
    "    \n",
    "        \n",
    "    # Remove the last two columns\n",
    "    \n",
    "    data.drop(data.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Calculate end-of-life cycles for each unit\n",
    "    \n",
    "    eol_cycles = pd.DataFrame(data.groupby('unit')['cycle'].max()).reset_index()\n",
    "    eol_cycles.columns = ['unit', 'eol_cycles']\n",
    "    \n",
    "    \n",
    "    # Calculate Remaining Useful Life (RUL) for each unit at each cycle\n",
    "\n",
    "    data = data.merge(eol_cycles, on=['unit'], how='left')\n",
    "    data['RUL'] = data['eol_cycles'] - data['cycle']\n",
    "    data.drop('eol_cycles', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Create label columns\n",
    "    w = 16\n",
    "    data['L1'] = np.where(data['RUL'] < w, 1, 0)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readDataset(dset=1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataset2(dset=1):\n",
    "\n",
    "    # Read data file\n",
    "\n",
    "    dtype='train'\n",
    "    file_name = '{}_FD00{}.txt'.format(dtype, str(dset))\n",
    "    columns = ['unit', 'cycle', 'os1', 'os2', 'os3'] + ['sm{}'.format(j+1) for j in range(23)]\n",
    "    \n",
    "    data = pd.read_csv('./CMAPSSData/{}'.format(file_name), delimiter=' ', names=columns)\n",
    "    \n",
    "        \n",
    "    # Remove the last two columns\n",
    "    \n",
    "    data.drop(data.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Calculate end-of-life cycles for each unit\n",
    "    \n",
    "    eol_cycles = pd.DataFrame(data.groupby('unit')['cycle'].max()).reset_index()\n",
    "    eol_cycles.columns = ['unit', 'eol_cycles']\n",
    "    \n",
    "    \n",
    "    # Calculate Remaining Useful Life (RUL) for each unit at each cycle\n",
    "\n",
    "    data = data.merge(eol_cycles, on=['unit'], how='left')\n",
    "    data['RUL'] = data['eol_cycles'] - data['cycle']\n",
    "    data.drop('eol_cycles', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Create label columns\n",
    "    w = 32\n",
    "    data['L1'] = np.where(data['RUL'] < w, 1, 0)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = readDataset2(dset=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window = 5\n",
    "data_smoothed = data.copy()\n",
    "\n",
    "for unit in np.arange(1, 101, 1):\n",
    "    for channel in np.arange(1, 22, 1): #measurement_channels:\n",
    "        smoothed_data = data.loc[data['unit'] == unit, 'sm{}'.format(channel)].rolling(smoothing_window).sum()/smoothing_window\n",
    "        data_smoothed.loc[data_smoothed['unit'] == unit, 'sm{}'.format(channel)] = smoothed_data\n",
    "        \n",
    "data_smoothed = data_smoothed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window = 5\n",
    "data_smoothed2 = data2.copy()\n",
    "\n",
    "for unit in np.arange(1, 101, 1):\n",
    "    for channel in np.arange(1, 22, 1): #measurement_channels:\n",
    "        smoothed_data2 = data2.loc[data['unit'] == unit, 'sm{}'.format(channel)].rolling(smoothing_window).sum()/smoothing_window\n",
    "        data_smoothed2.loc[data_smoothed2['unit'] == unit, 'sm{}'.format(channel)] = smoothed_data2\n",
    "        \n",
    "data_smoothed2 = data_smoothed2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement_channels = [11, 14]\n",
    "# # measurement_channels = [7, 11, 14]\n",
    "# # measurement_channels = [7, 11, 12, 13, 14]\n",
    "# training_units = np.arange(1, 81, 1)\n",
    "# validation_units = np.array(list(set(np.arange(1, 101, 1)).difference(training_units)))\n",
    "# data_shift = 1 # Change this for data_shift-step ahead prediction\n",
    "\n",
    "# training_data = data_smoothed.loc[data_smoothed['unit'].isin(training_units)]\n",
    "# validation_data = data_smoothed.loc[~data_smoothed['unit'].isin(training_units)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the training set: 348/15818 (2.20%)\n",
      "\n",
      "Number of mislabeled points in the validation set: 102/4413 (2.31%)\n"
     ]
    }
   ],
   "source": [
    "measurement_channels = [7, 11, 14]\n",
    "\n",
    "training_units = np.arange(1, 81, 1)\n",
    "validation_units = np.array(list(set(np.arange(1, 101, 1)).difference(training_units)))\n",
    "\n",
    "data_shift = 1 # Change this for N-step ahead prediction\n",
    "\n",
    "\n",
    "def sliceData(data, channels, units):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get a slice of the data set corresponding to specific channels and units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "        The data to be sliced.\n",
    "    channels: list, tuple or ndarray\n",
    "        The list of channels to be sliced.\n",
    "    units: list, tuple of ndarray\n",
    "        The list of units to be sliced.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The sliced input and output data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    sliced_data = data.loc[data['unit'].isin(units)]\n",
    "    X = sliced_data[['sm{}'.format(j) for j in channels]]\n",
    "    Y = sliced_data['L1']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def shiftData(X, Y, shift):\n",
    "\n",
    "    \"\"\"\n",
    "    Shift input and output data for N-step ahead predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The input and output data sets.\n",
    "    shift: int\n",
    "        The number of shifts\n",
    "    \"\"\"\n",
    "\n",
    "    X = X[:-shift].to_numpy()\n",
    "    Y = Y[shift:].to_numpy()\n",
    "    \n",
    "    rows_to_delete = np.where(Y == 1)[0][:-1] + data_shift\n",
    "    X = np.delete(X, rows_to_delete, axis=0)\n",
    "    Y = np.delete(Y, rows_to_delete)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "training_data = data_smoothed.loc[data_smoothed['unit'].isin(training_units)]\n",
    "validation_data = data_smoothed.loc[~data_smoothed['unit'].isin(training_units)]\n",
    "\n",
    "X_training, Y_training = sliceData(data_smoothed, measurement_channels, training_units)\n",
    "X_validation, Y_validation = sliceData(data_smoothed, measurement_channels, validation_units)\n",
    "\n",
    "transformer = MinMaxScaler().fit(X_training)\n",
    "# transformer = MaxAbsScaler().fit(X_training)\n",
    "X_training = transformer.transform(X_training)\n",
    "X_validation = transformer.transform(X_validation)\n",
    "\n",
    "\n",
    "# Train and evaluate the model using the training set\n",
    "\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=25)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=6)\n",
    "dtc = tree.DecisionTreeClassifier(max_depth=4)\n",
    "Y_predicted = dtc.fit(X_training, Y_training).predict(X_training)\n",
    "\n",
    "total = X_training.shape[0]\n",
    "mislabeled = (Y_training != Y_predicted).sum()\n",
    "message = 'Number of mislabeled points in the training set: {}/{} ({:.2f}%)\\n'\n",
    "print(message.format(mislabeled, total, mislabeled/total*100))\n",
    "\n",
    "# Evaluate the model using the validation set\n",
    "\n",
    "Y_predicted = dtc.predict(X_validation)\n",
    "\n",
    "total = X_validation.shape[0]\n",
    "mislabeled = (Y_validation != Y_predicted).sum()\n",
    "message = 'Number of mislabeled points in the validation set: {}/{} ({:.2f}%)'\n",
    "print(message.format(mislabeled, total, mislabeled/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the training set: 644/15818 (4.07%)\n",
      "\n",
      "Number of mislabeled points in the validation set: 177/4413 (4.01%)\n"
     ]
    }
   ],
   "source": [
    "measurement_channels = [7, 11, 14]\n",
    "\n",
    "training_units = np.arange(1, 81, 1)\n",
    "validation_units = np.array(list(set(np.arange(1, 101, 1)).difference(training_units)))\n",
    "\n",
    "data_shift = 1 # Change this for N-step ahead prediction\n",
    "\n",
    "\n",
    "def sliceData(data, channels, units):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get a slice of the data set corresponding to specific channels and units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "        The data to be sliced.\n",
    "    channels: list, tuple or ndarray\n",
    "        The list of channels to be sliced.\n",
    "    units: list, tuple of ndarray\n",
    "        The list of units to be sliced.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The sliced input and output data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    sliced_data = data.loc[data['unit'].isin(units)]\n",
    "    X = sliced_data[['sm{}'.format(j) for j in channels]]\n",
    "    Y = sliced_data['L1']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def shiftData(X, Y, shift):\n",
    "\n",
    "    \"\"\"\n",
    "    Shift input and output data for N-step ahead predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y: pandas.DataFrame\n",
    "        The input and output data sets.\n",
    "    shift: int\n",
    "        The number of shifts\n",
    "    \"\"\"\n",
    "\n",
    "    X = X[:-shift].to_numpy()\n",
    "    Y = Y[shift:].to_numpy()\n",
    "    \n",
    "    rows_to_delete = np.where(Y == 1)[0][:-1] + data_shift\n",
    "    X = np.delete(X, rows_to_delete, axis=0)\n",
    "    Y = np.delete(Y, rows_to_delete)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "training_data2 = data_smoothed2.loc[data_smoothed2['unit'].isin(training_units)]\n",
    "validation_data2 = data_smoothed2.loc[~data_smoothed2['unit'].isin(training_units)]\n",
    "\n",
    "X_training2, Y_training2 = sliceData(data_smoothed2, measurement_channels, training_units)\n",
    "X_validation2, Y_validation2 = sliceData(data_smoothed2, measurement_channels, validation_units)\n",
    "\n",
    "transformer2 = MinMaxScaler().fit(X_training2)\n",
    "# transformer = MaxAbsScaler().fit(X_training)\n",
    "X_training2 = transformer2.transform(X_training2)\n",
    "X_validation2 = transformer2.transform(X_validation2)\n",
    "\n",
    "\n",
    "# Train and evaluate the model using the training set\n",
    "\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=25)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# dtc = tree.DecisionTreeClassifier(criterion='entropy', max_depth=6)\n",
    "dtc2 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "Y_predicted2 = dtc2.fit(X_training2, Y_training2).predict(X_training2)\n",
    "\n",
    "total2 = X_training2.shape[0]\n",
    "mislabeled2 = (Y_training2 != Y_predicted2).sum()\n",
    "message = 'Number of mislabeled points in the training set: {}/{} ({:.2f}%)\\n'\n",
    "print(message.format(mislabeled2, total2, mislabeled2/total2*100))\n",
    "\n",
    "# Evaluate the model using the validation set\n",
    "\n",
    "Y_predicted2 = dtc2.predict(X_validation2)\n",
    "\n",
    "total2 = X_validation2.shape[0]\n",
    "mislabeled2 = (Y_validation2 != Y_predicted2).sum()\n",
    "message = 'Number of mislabeled points in the validation set: {}/{} ({:.2f}%)'\n",
    "print(message.format(mislabeled2, total2, mislabeled2/total2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decisions = np.arange(0,400,10) # decisions can only be made every DT = 10 cycles\n",
    "C_p = 100\n",
    "C_c = 135\n",
    "DT  = 10  # Decisions can be taken every DT=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "###### Find the optimal expected cost rate wrt the TF distribution.##############################\n",
    "#################################################################################################\n",
    "\n",
    "TF_pdf = lambda x: sps.norm.pdf(x, loc = 206.11, scale = 46.11)\n",
    "\n",
    "def objective_1(trep):\n",
    "\n",
    "    Ppr_integrand = lambda y: TF_pdf(y)\n",
    "    Ppr = integrate.quad(Ppr_integrand, trep, np.inf)[0]\n",
    "\n",
    "    numerator = Ppr * C_p + (1-Ppr) * C_c\n",
    "\n",
    "    TF_integrand = lambda y: y * TF_pdf(y)\n",
    "    TF_estimate = integrate.quad(TF_integrand, 0, trep)[0]\n",
    "\n",
    "    denominator =  Ppr * trep + TF_estimate\n",
    "\n",
    "    return numerator/denominator\n",
    "\n",
    "minimization = least_squares(objective_1, x0=206.11, bounds=(0, 400))\n",
    "\n",
    "# minimization = minimize(objective,  x0=225.3, method = 'Nelder-Mead')\n",
    "trep_star    = minimization.x[0]\n",
    "cost_star    = minimization.fun[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208.02945474663812, 0.62589820660972)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trep_star, cost_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Covariance of the parameters could not be estimated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_array = np.zeros(20)\n",
    "t_LC_array  = np.zeros(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit: 81 , preventive replacement informed at cycle: 230.0\n",
      "component lifecycle: 230.0\n",
      "true failure time: 240\n",
      "------------------------------------------------------\n",
      "Unit: 82 , preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "true failure time: 214\n",
      "------------------------------------------------------\n",
      "Unit: 83 , preventive replacement informed at cycle: 290.0\n",
      "component lifecycle: 290.0\n",
      "true failure time: 293\n",
      "------------------------------------------------------\n",
      "Unit: 84 , preventive replacement informed at cycle: 250.0\n",
      "component lifecycle: 250.0\n",
      "true failure time: 267\n",
      "------------------------------------------------------\n",
      "Unit: 85 , preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "true failure time: 188\n",
      "------------------------------------------------------\n",
      "Unit: 86 , preventive replacement informed at cycle: 250.0\n",
      "component lifecycle: 250.0\n",
      "true failure time: 278\n",
      "------------------------------------------------------\n",
      "Unit: 87 , preventive replacement informed at cycle: 170.0\n",
      "component lifecycle: 170.0\n",
      "true failure time: 178\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1953: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit: 88 , preventive replacement informed at cycle: 210.0\n",
      "component lifecycle: 210.0\n",
      "true failure time: 213\n",
      "------------------------------------------------------\n",
      "Unit: 89 , preventive replacement informed at cycle: 200.0\n",
      "component lifecycle: 200.0\n",
      "true failure time: 217\n",
      "------------------------------------------------------\n",
      "Unit: 90 , preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "true failure time: 154\n",
      "------------------------------------------------------\n",
      "Unit: 91 , preventive replacement informed at cycle: 110.0\n",
      "component lifecycle: 110.0\n",
      "true failure time: 135\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1953: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit: 92 , preventive replacement informed at cycle: 320.0\n",
      "component lifecycle: 320.0\n",
      "true failure time: 341\n",
      "------------------------------------------------------\n",
      "Unit: 93 , preventive replacement informed at cycle: 150.0\n",
      "component lifecycle: 150.0\n",
      "true failure time: 155\n",
      "------------------------------------------------------\n",
      "Unit: 94 , preventive replacement informed at cycle: 240.0\n",
      "component lifecycle: 240.0\n",
      "true failure time: 258\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1953: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit: 95 , preventive replacement informed at cycle: 280.0\n",
      "component lifecycle: 280.0\n",
      "true failure time: 283\n",
      "------------------------------------------------------\n",
      "Unit: 96 , preventive replacement informed at cycle: 310.0\n",
      "component lifecycle: 310.0\n",
      "true failure time: 336\n",
      "------------------------------------------------------\n",
      "Unit: 97 , preventive replacement informed at cycle: 180.0\n",
      "component lifecycle: 180.0\n",
      "true failure time: 202\n",
      "------------------------------------------------------\n",
      "Unit: 98 , preventive replacement informed at cycle: 140.0\n",
      "component lifecycle: 140.0\n",
      "true failure time: 156\n",
      "------------------------------------------------------\n",
      "Unit: 99 , preventive replacement informed at cycle: 170.0\n",
      "component lifecycle: 170.0\n",
      "true failure time: 185\n",
      "------------------------------------------------------\n",
      "Unit: 100 , preventive replacement informed at cycle: 190.0\n",
      "component lifecycle: 190.0\n",
      "true failure time: 200\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for unit in validation_data['unit'].unique():\n",
    "    \n",
    "    preventive_replacement = False\n",
    "\n",
    "    X = validation_data[['sm{}'.format(j) for j in measurement_channels]].loc[validation_data['unit'] == unit].to_numpy()\n",
    "    X = transformer.transform(X)\n",
    "    Y = validation_data['L1'].loc[validation_data['unit'] == unit].to_numpy()\n",
    "    \n",
    "    X2 = validation_data2[['sm{}'.format(j) for j in measurement_channels]].loc[validation_data2['unit'] == unit].to_numpy()\n",
    "    X2 = transformer2.transform(X2)\n",
    "    Y2 = validation_data2['L1'].loc[validation_data2['unit'] == unit].to_numpy()\n",
    "    \n",
    "    for cycle in range(validation_data[validation_data['unit']==unit].shape[0]):\n",
    "\n",
    "        if smoothing_window + cycle in array_decisions:\n",
    "\n",
    "            prob_RUL_smaller_DT = dtc.predict_proba(X[cycle].reshape(1,3))[0,1]\n",
    "            prob_RUL_smaller_2DT = dtc2.predict_proba(X2[cycle].reshape(1,3))[0,1]\n",
    "            \n",
    "            if prob_RUL_smaller_2DT<=0.00285:\n",
    "                continue\n",
    "            else:            \n",
    "#                 print(prob_RUL_smaller_DT,prob_RUL_smaller_2DT)\n",
    "\n",
    "                data_x = np.array([16, 32])\n",
    "                data_y = np.array([prob_RUL_smaller_DT, prob_RUL_smaller_2DT])\n",
    "\n",
    "                def fit_lognormalCDF(x, shape, loc, scale):\n",
    "                    return sps.lognorm.cdf(x, shape, 0, scale)\n",
    "\n",
    "                shape_LN, loc_LN, scale_LN = curve_fit(fit_lognormalCDF, data_x, data_y, method = 'dogbox')[0] \n",
    "                RUL_pred_k_pdf = lambda x: sps.lognorm.pdf(x, s = shape_LN, loc = loc_LN, scale = scale_LN)   \n",
    "                time_tk = smoothing_window + cycle\n",
    "\n",
    "#                 x = np.arange(0,100,0.1)        \n",
    "#                 plt.plot(x, RUL_pred_k_pdf(x))\n",
    "#                 plt.show()\n",
    "\n",
    "                def objective(trep_k):\n",
    "                    Ppr_integrand = lambda y: RUL_pred_k_pdf(y)\n",
    "                    Ppr = integrate.quad(Ppr_integrand, trep_k, np.inf)[0]\n",
    "                    part_1 = Ppr * C_p + (1-Ppr) * C_c\n",
    "    #                 part_2_integrand = lambda y: (y-trep_k) * RUL_pred_k_pdf(y) * (cost_star+100/206.11)/2\n",
    "                    part_2_integrand = lambda y: (y-trep_k) * RUL_pred_k_pdf(y) * cost_star\n",
    "                    part_2 = integrate.quad(part_2_integrand, trep_k, np.inf)[0]\n",
    "                    return part_1 + part_2\n",
    "\n",
    "                minimization = least_squares(objective, x0=20, bounds=(0, 400))\n",
    "                trep_k_star = minimization.x[0]\n",
    "\n",
    "            # evaluate decision heuristics\n",
    "            if trep_k_star < 10:\n",
    "                t_LC_array[counter] = smoothing_window+cycle\n",
    "                costs_array[counter] = C_p\n",
    "                print('Unit:', unit, ', preventive replacement informed at cycle:', t_LC_array[counter])\n",
    "                print('component lifecycle:', t_LC_array[counter])\n",
    "                preventive_replacement = True\n",
    "                break\n",
    "\n",
    "    if preventive_replacement == False:\n",
    "        \n",
    "        t_LC_array[counter] = validation_data[validation_data['unit']==unit]['cycle'].iloc[-1]\n",
    "        print('Unit:', unit, ', component failure at t:', t_LC_array[counter])\n",
    "        costs_array[counter] = C_c\n",
    "\n",
    "    print('true failure time:', validation_data[validation_data['unit']==unit]['cycle'].iloc[-1] )\n",
    "    print('------------------------------------------------------')\n",
    "        \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230., 200., 290., 250., 180., 250., 170., 210., 200., 150., 110.,\n",
       "       320., 150., 240., 280., 310., 180., 140., 170., 190.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LC_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_LC_perfect_array = np.array([240., 210., 290., 260., 180., 270., 170., 210., 210., 150., 130.,\n",
    "       340., 150., 250., 280., 330., 200., 150., 180., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_cost_perfect = 100 / np.mean(t_LC_perfect_array)\n",
    "expected_cost_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47393364928909953"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of the expected cost per unit time, Eqns. (3) and (4) of our paper.\n",
    "expected_cost_DT = np.mean(costs_array)/np.mean(t_LC_array)\n",
    "expected_cost_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04265402843601901"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation of the metric defined in the paper\n",
    "M = (expected_cost_DT - expected_cost_perfect) / expected_cost_perfect\n",
    "M # it obtains a very small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.265402843601901"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BsYPkY7gYGx",
    "ElhakcHtnX9J"
   ],
   "name": "Predictive-Maintenance-using-LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
